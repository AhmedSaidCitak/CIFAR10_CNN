{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55a5d42-1e7b-453a-9217-a65c0ecb1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9dd4944-fb9e-4ee0-a08f-e6f9da8a8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "else:\n",
    "    print('CUDA is not available.  Training on CPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46e172e-9a0b-4d3f-97eb-fc672cca890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5758b1-54ed-4c3b-9bf7-0fee52354913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the CNN architecture\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 16 * 5 * 5)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f42c37-20d4-44c9-a276-f2a2c1c81aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d26526-9c7b-45fe-8481-e7f1b00206db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.277122 \tValidation Loss: 0.281484 \tPatience Counter: 0\n",
      "Validation loss decreased (inf --> 0.281484).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.036301 \tValidation Loss: 0.243558 \tPatience Counter: 0\n",
      "Validation loss decreased (0.281484 --> 0.243558).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.930643 \tValidation Loss: 0.228076 \tPatience Counter: 0\n",
      "Validation loss decreased (0.243558 --> 0.228076).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.859684 \tValidation Loss: 0.226683 \tPatience Counter: 0\n",
      "Validation loss decreased (0.228076 --> 0.226683).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.811182 \tValidation Loss: 0.227679 \tPatience Counter: 0\n",
      "Epoch: 5 \tTraining Loss: 0.770852 \tValidation Loss: 0.218155 \tPatience Counter: 1\n",
      "Validation loss decreased (0.226683 --> 0.218155).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.734810 \tValidation Loss: 0.229320 \tPatience Counter: 0\n",
      "Epoch: 7 \tTraining Loss: 0.702663 \tValidation Loss: 0.217738 \tPatience Counter: 1\n",
      "Validation loss decreased (0.218155 --> 0.217738).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.673224 \tValidation Loss: 0.225408 \tPatience Counter: 0\n",
      "Epoch: 9 \tTraining Loss: 0.642488 \tValidation Loss: 0.216879 \tPatience Counter: 1\n",
      "Validation loss decreased (0.217738 --> 0.216879).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.616041 \tValidation Loss: 0.225657 \tPatience Counter: 0\n",
      "Epoch: 11 \tTraining Loss: 0.596162 \tValidation Loss: 0.226946 \tPatience Counter: 1\n",
      "Epoch: 12 \tTraining Loss: 0.572713 \tValidation Loss: 0.236911 \tPatience Counter: 2\n",
      "Epoch: 13 \tTraining Loss: 0.554939 \tValidation Loss: 0.235032 \tPatience Counter: 3\n",
      "Epoch: 14 \tTraining Loss: 0.534228 \tValidation Loss: 0.239028 \tPatience Counter: 4\n",
      "Epoch: 15 \tTraining Loss: 0.519473 \tValidation Loss: 0.246621 \tPatience Counter: 5\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1000\n",
    "#List to store loss to visualize\n",
    "train_losslist = []\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    train_losslist.append(train_loss)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tPatience Counter: {}'.format(\n",
    "        epoch, train_loss, valid_loss, patience_counter))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        patience_counter = 0\n",
    "    elif patience_counter < patience:\n",
    "        patience_counter += 1\n",
    "    else:\n",
    "        break  \n",
    "        \n",
    "plt.plot(n_epochs, train_losslist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Performance of Model 1\")\n",
    "plt.show()\n",
    "\n",
    "model.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e06585-86c0-4f06-be8d-b80f2db8ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
